---
title: "Apache Arrow 鬼はええ！<br/>このままCSV全部Parquetに<br/>変換していこうぜ！"
subtitle: "2022-03-19 第97回R勉強会@東京<br/>@eitsupi"
format:
  revealjs:
    theme: [night, custom.scss]
    width: 1280
    height: 720
    slide-number: true
    chalkboard: false
    preview-links: auto
    footer: "#TokyoR"
    self-contained: false
engine: knitr
---

# はじめに

## 自己紹介

:::: {.columns}
::: {.column width="30%"}

![](../image/eitsupi.jpg){fig-align="center" width="300" height="300"}

:::
::: {.column width="70%"}

- [@eitsupi](https://twitter.com/eitsupi)
- 製造業勤務
  - Excelが嫌になりRを触り初めて3年
- Dockerイメージ`rocker/r-ver`他のメンテナー

:::
::::

## 今日の話……

数十分かけて読み込んでいたCSVファイル群をParquetに置換してみたら数分で読めるようになったのでその際に調べたこと

:::: {.columns}
::: {.column width="50%"}

### 対象かも

✅データはCSVファイル  
✅大量のファイルを読む  
✅読み込みに数十分かかる

:::
::: {.column width="50%"}

### 対象外かも

✅データはDB上  
✅少数のファイルを読む  
✅読み込みは数秒で終わる

:::
::::

## 結論

Q. CSVをParquetにするとどのくらい早くなる？

A. 場合による（ようなので試してみましょう！）

# Apache Parquet と<br/>Apache Arrow

## Apache Parquet

- 2013年～[^1]
- Apache Hadoop用に作られた**列指向**の**ファイルフォーマット**
  - 列方向に圧縮されるため大量のレコードを圧縮しやすい
  - 列単位でベクトル化した計算を行う分析処理と相性が良い

[^1]: [Announcing Parquet 1.0: Columnar Storage for Hadoop](https://blog.twitter.com/engineering/en_us/a/2013/announcing-parquet-10-columnar-storage-for-hadoop)

## Apache Arrow

- 2016年～[^2]
- _A high-performance cross-system data layer for columnar in-memory analytics_
- 言語に寄らない**列指向**の**インメモリフォーマット**の標準を目指しているプロジェクト
  - Arrowを介することで、ある列指向データから別の列指向データへの変換を個別に実装する必要はなくなる

[^2]: [The Apache® Software Foundation Announces Apache Arrow™ as a Top-Level Project](https://blogs.apache.org/foundation/entry/the_apache_software_foundation_announces87)

## ParquetとFeather

- 2016年時点でArrowはファイル形式を提供しなかったため、ファイル形式およびそのファイルを読み書きする[Featherライブラリ](https://github.com/wesm/feather)が試験的に作られる
- 2020年にArrow IPC (Inter-Process Communication)フォーマットをLZ4かZSTDで圧縮した通称FeatherV2がArrow本体に組み込まれる[^3]
  - 読み書きする関数名は`feather`だったり`arrow`だったり……
  - 2021年に決まった正式な拡張子は`.arrow`？[^4]
- 一方2017年にはParquetをArrowライブラリで読み書きできるようになっており、一般的にはParquetの利用が推奨されている[^5]

[^3]: [Apache Arrow 0.17.0 Release](https://arrow.apache.org/blog/2020/04/21/0.17.0-release/)
[^4]: [Apache Arrowデータのメディアタイプ（MIMEタイプ）](https://www.clear-code.com/blog/2021/6/25/apache-arrow-media-types.html)
[^5]: [Feather V2 with Compression Support in Apache Arrow 0.17.0](https://ursalabs.org/blog/2020-feather-v2/)

## Arrow R Package

- パッケージ名は`arrow`
- Apache Arrow C++ライブラリ（`libarrow`）のRバインディング
  - Python
- ソースインストールすると`libarrow`のビルドに長時間がかかることに注意！
- RockerプロジェクトのDockerでは`rocker/tidyverse`にインストール済（のでこのスライド内のサンプルコードは`rocker/tidyverse`で動くはず）

# `arrow`によるファイル読み込み

## ファイルの読み書き

`arrow`パッケージは独自にファイルを読み書きする関数を持っている

| 対象ファイル | 数   |  utils   |                               readr                               |                                       arrow                                       |
| ------------ | ---- | :------: | :---------------------------------------------------------------: | :-------------------------------------------------------------------------------: |
| csv          | 単体 | read.csv | [read_csv](https://readr.tidyverse.org/reference/read_delim.html) | [read_csv_arrow](https://arrow.apache.org/docs/r/reference/read_delim_arrow.html) |
| csv          | 複数 |    -     |                             read_csv                              |    [open_dataset](https://arrow.apache.org/docs/r/reference/open_dataset.html)    |
| parquet      | 単体 |    -     |                                 -                                 |    [read_parquet](https://arrow.apache.org/docs/r/reference/read_parquet.html)    |
| parquet      | 複数 |    -     |                                 -                                 |                                   open_dataset                                    |

: データ読み込み関数の比較 {#tbl-readfiles}

## CSVの読み込み　1/3

```{r}
#| echo: true
#| code-line-numbers: "|4"
file_csv <- readr::readr_example("mtcars.csv")

file_csv |>
  arrow::read_csv_arrow(as_data_frame = FALSE)
```

::: {.incremental}
- `as_data_frame`引数はRの`data.frame`に変換するか`arrow::Table`のままにするかを制御する（デフォルトは`TRUE`）
:::

## CSVの読み込み　2/3

同じ列構造を持つ複数ファイルを読み込みたい場合は`open_dataset()`によりデータセットとして開く

```{r}
#| echo: true
c(file_csv, file_csv) |> arrow::open_dataset(format = "csv")
```

この段階では矩形データの構造（スキーマ）を読み込んだだけで、データ全体を読み込んではいない

## CSVの読み込み　3/3

`compute`か`collect`でデータをメモリ上に読み込む

```{r}
#| echo: true
ds <- c(file_csv, file_csv) |> arrow::open_dataset(format = "csv")
```

::::{.columns}
:::{.column width="50%"}

### Table

```{r}
#| echo: true
ds |> dplyr::compute()
```

:::
:::{.column width="50%"}

### data.frame

```{r}
#| echo: true
ds |> dplyr::collect()
```
:::
::::

## Parquetファイルの読み込み　1/2

`write_parquet`でParquetファイルを書き込んで、`read_parquet`で読み込む

```{r}
#| echo: true
readr::readr_example("mtcars.csv") |>
  arrow::read_csv_arrow(as_data_frame = FALSE) |>
  arrow::write_parquet("mtcars.parquet")

arrow::read_parquet("mtcars.parquet", as_data_frame = FALSE)
```

## Parquetファイルの読み込み　2/2

Parquetの場合も複数ファイルの場合は`open_dataset`で開く`format`引数のデフォルトは`"parquet"`なので指定しなくてもよい

```{r}
#| echo: true
c("mtcars.parquet", "mtcars.parquet") |>
  arrow::open_dataset(format = "parquet") |>
  dplyr::compute()
```

# dplyr query

## dplyrの基本

[`dplyr`](https://dplyr.tidyverse.org/)の中心的な関数は`data.frame`を第一引数にとり`data.frame`を返す

```{r}
#| echo: true
mtcars |>
  dplyr::select(cyl) |>
  class()
```

## arrow_dplyr_query

`Table`や`Dataset`をdplyrのクエリの対象にすると`arrow_dplyr_query`クラスオブジェクトになる

```{r}
#| echo: true
arrow::open_dataset("mtcars.parquet") |>
  dplyr::select(cyl) |>
  class()
```

::: {.incremental}
- `arrow_dplyr_query`を`compute`か`collect`に渡すことでクエリが実行される（`dbplyr`に類似）
- Rの`data.frame`に変換することなくArrowインメモリフォーマットのままdplyrで記述した処理を実行できる
:::

## 遅延評価 {auto-animate="true"}

クエリは`compute`もしくは`collect`に繋げるまで評価されない

```r
c("mtcars.parquet", "mtcars.parquet") |>
  arrow::open_dataset(format = "parquet") |>
  dplyr::collect()
```

## 遅延評価 {auto-animate="true"}

クエリは`compute`もしくは`collect`に繋げるまで評価されない

```{.r code-line-numbers="|3-4"}
c("mtcars.parquet", "mtcars.parquet") |>
  arrow::open_dataset(format = "parquet") |>
  dplyr::filter(cyl == 4) |>
  dplyr::select(dplyr::starts_with("d")) |>
  dplyr::collect()
```

::: {.incremental}
- dplyrクエリはarrowパッケージによって翻訳され`libarrow`がクエリを実行する
- 翻訳可能な関数はarrowに登録されているもののみなので、非対応の関数を含めるとエラーになる（データセットに対するクエリの場合）
- 対応している関数は徐々に増えており、[NEWS](https://arrow.apache.org/docs/r/news/index.html)で確認可能
:::

## プッシュダウン {auto-animate="true"}

クエリは`compute`もしくは`collect`に繋げるまで評価されない

```{.r code-line-numbers="1-2"}
c("mtcars.parquet", "mtcars.parquet") |>
  arrow::open_dataset(format = "parquet") |>
  dplyr::filter(cyl == 4) |>
  dplyr::select(dplyr::starts_with("d")) |>
  dplyr::collect()
```

::: {.incremental}
- Parquetデータセットに対してクエリを実行するとき、クエリを解析し必要な列と行のみをファイルから読み込む（**プッシュダウン**）
  - 読み込むデータが少なくなるほど読み込み時間は短縮される
  - **CSVとParquetの大きな差**
:::

## 実行 {auto-animate="true"}

クエリは`compute`もしくは`collect`に繋げるまで評価されない

```{r}
#| echo: true
c("mtcars.parquet", "mtcars.parquet") |>
  arrow::open_dataset(format = "parquet") |>
  dplyr::filter(cyl == 4) |>
  dplyr::select(dplyr::starts_with("d")) |>
  dplyr::collect()
```

## メモリに乗り切らないデータセットの加工



```{.r code-line-numbers="|5-6"}
fs::dir_create("test_data")

c("mtcars.parquet", "mtcars.parquet") |>
  arrow::open_dataset(format = "parquet") |>
  dplyr::group_by(cyl) |>
  arrow::write_dataset("test_data")
```

```shell
$ tree test_data
test_data
├── cyl=4
│   └── part-0.parquet
├── cyl=6
│   └── part-0.parquet
└── cyl=8
    └── part-0.parquet

3 directories, 3 files
```

<!-- clean up -->

```{r}
fs::file_delete("mtcars.parquet")
```

# Parquetの注意点

- 対応ツールが少ない
- 特殊な形式に注意
  - 独自メタデータ
  - Hive-style パーティショニング
